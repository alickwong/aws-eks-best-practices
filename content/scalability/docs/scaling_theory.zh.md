
# Kubernetes 扩展理论

## 节点与变化率
通常在讨论 Kubernetes 的可扩展性时,我们会以集群中节点的数量为依据。有趣的是,这通常并不是理解可扩展性最有用的指标。例如,一个有 5,000 个节点但 pod 数量固定的集群,在初始设置后不会给控制平面带来太大压力。但是,如果我们有一个 1,000 个节点的集群,在不到一分钟内尝试创建 10,000 个短暂的作业,这将给控制平面带来持续的压力。

仅仅使用节点数量来理解扩展性可能会产生误导。更好的方法是考虑在特定时间间隔内(我们以 5 分钟为例,因为这是 Prometheus 查询的默认设置)发生变化的速率。让我们探讨一下为什么从变化率的角度来定义问题可以让我们更好地了解需要调整的内容以实现所需的扩展。

## 以查询每秒为思考方式
Kubernetes 为每个组件(Kubelet、调度器、Kube 控制器管理器和 API 服务器)都设有一些保护机制,以防止压垮 Kubernetes 链条中的下一个环节。例如,Kubelet 有一个标志可以限制对 API 服务器的调用速率。这些保护机制通常(但并非总是)以每秒查询数(QPS)的形式表达。

在更改这些 QPS 设置时必须格外小心。消除一个瓶颈(如 Kubelet 的每秒查询数)将影响其他下游组件。这可能会在超过某个速率时使整个系统不堪重负,因此了解和监控 Kubernetes 服务链中的每个部分对于成功扩展工作负载至关重要。

!!! note 
    API 服务器有一个更复杂的系统,引入了 API 优先级和公平性,我们将单独讨论这一点。

!!! note 
    小心,有些指标看起来很合适,但实际上在衡量其他内容。例如,`kubelet_http_inflight_requests` 只与 Kubelet 中的指标服务器有关,而不是 Kubelet 到 apiserver 的请求数。这可能会导致我们错误地配置 Kubelet 上的 QPS 标志。查看特定 Kubelet 的审核日志会是一种更可靠的方式来检查指标。

## 扩展分布式组件

由于 EKS 是一项托管服务,让我们将 Kubernetes 组件分为两类:AWS 托管组件,包括 etcd、Kube Controller Manager 和调度程序(图表左侧),以及客户可配置组件,如 Kubelet、容器运行时和调用 AWS API 的各种操作员,如网络和存储驱动程序(图表右侧)。我们将 API 服务器放在中间,尽管它是 AWS 托管的,因为 API 优先级和公平性的设置可以由客户配置。

![Kubernetes 组件](../images/k8s-components.png)

## 上游和下游瓶颈
在监控每项服务时,我们需要查看两个方向的指标,以找出瓶颈。让我们以 Kubelet 为例来学习如何做到这一点。Kubelet 同时与 API 服务器和容器运行时进行通信;我们需要监控**如何**和**什么**,以检测这两个组件是否存在问题?

### 每个节点的 Pod 数量
当我们查看扩展数字,如每个节点可以运行多少个 Pod 时,我们可以接受上游支持的 110 个 Pod 每个节点的数值。

!!! note
    https://kubernetes.io/docs/setup/best-practices/cluster-large/

然而,您的工作负载可能比在上游可扩展性测试中使用的更复杂。为了确保我们可以服务于我们想要在生产中运行的 Pod 数量,让我们确保 Kubelet 能够"跟上"Containerd 运行时。

![跟上](../images/keeping-up.png)

为了简单起见,Kubelet 正在从容器运行时(在我们的例子中是 Containerd)获取 Pod 的状态。如果我们有太多 Pod 状态变化得太快怎么办?如果变化速率太高,请求[到容器运行时]可能会超时。

!!! note 
    Kubernetes 正在不断发展,这个子系统目前正在经历变化。https://github.com/kubernetes/enhancements/issues/3386

![流程](../images/flow.png)
![PLEG 持续时间](../images/PLEG-duration.png)

在上图中,我们看到一条平坦的线,表示我们刚刚达到了 Pod 生命周期事件生成持续时间指标的超时值。如果您想在自己的集群中看到这一点,您可以使用以下 PromQL 语法。
```
increase(kubelet_pleg_relist_duration_seconds_bucket{instance="$instance"}[$__rate_interval])
```


如果我们目睹了这种超时行为,我们就知道我们已经将节点推到了它所能承受的极限。我们需要在继续之前解决超时的根本原因。这可以通过减少每个节点上的 pod 数量,或者寻找可能导致高重试量的错误(从而影响吞吐率)来实现。重要的是要认识到,指标是了解节点是否能够处理分配给它的 pod 的吞吐率的最佳方式,而不是使用固定的数字。

## 根据指标进行扩展
尽管使用指标来优化系统的概念已经很古老了,但在人们开始他们的 Kubernetes 之旅时,它往往会被忽视。我们不是专注于特定的数字(即每个节点 110 个 pod),而是将我们的努力集中在找到能帮助我们发现系统瓶颈的指标上。了解这些指标的正确阈值可以让我们高度确信我们的系统得到了最佳配置。

### 变更的影响
一个可能让我们陷入麻烦的常见模式是关注第一个看起来可疑的指标或日志错误。当我们发现 Kubelet 出现超时时,我们可以尝试一些随机的事情,比如增加 Kubelet 每秒钟被允许发送的速率等。然而,明智的做法是首先查看我们发现的错误下游的整体情况。*每次变更都要有目的,并有数据支持*。

Kubelet 的下游包括 Containerd 运行时(pod 错误)、DaemonSets 如存储驱动程序(CSI)和网络驱动程序(CNI),它们与 EC2 API 进行通信等。

![Flow add-ons](../images/flow-addons.png)

让我们继续之前的例子,Kubelet 跟不上运行时。在这里,我们可能会在节点上打包得太密集,从而触发错误。

![Bottlenecks](../images/bottlenecks.png)

在为我们的工作负载设计合适的节点大小时,这些容易被忽视的信号可能会给系统施加不必要的压力,从而限制我们的扩展性和性能。

### 不必要错误的代价

Kubernetes 控制器擅长在出现错误条件时进行重试,但这也是有代价的。这些重试可能会增加诸如 Kube Controller Manager 等组件的压力。监控此类错误是进行扩展测试的一个重要原则。

当出现较少错误时,更容易发现系统中的问题。通过定期确保我们的集群在重大操作(如升级)之前是无错误的,我们可以简化在发生意外事件时的故障排查日志。

#### 扩展我们的视野
在拥有数千个节点的大规模集群中,我们不希望逐个查找瓶颈。在 PromQL 中,我们可以使用名为 topk 的函数来查找数据集中的最高值;K 是我们要获取的项目数量。在这里,我们使用三个节点来了解集群中的所有 Kubelet 是否都已饱和。到目前为止,我们一直在关注延迟,现在让我们看看 Kubelet 是否正在丢弃事件。
```
topk(3, increase(kubelet_pleg_discard_events{}[$__rate_interval]))
```

[分解这个陈述。

* 我们使用 Grafana 变量 `$__rate_interval` 来确保它获取所需的四个样本。这绕过了监控中的一个复杂主题,使用一个简单的变量。
* `topk` 给我们只有最高结果,数字 3 将这些结果限制为三个。这是集群范围指标的一个有用功能。
* `{}` 告诉我们没有过滤器,通常您会放置任何抓取规则的作业名称,但由于这些名称各不相同,我们将其留空。

#### 将问题一分为二

为了解决系统中的瓶颈,我们将采取寻找一个指标的方法,该指标显示我们存在上游或下游问题,因为这允许我们将问题一分为二。这也将是我们展示指标数据的核心原则之一。

这个过程的一个好的起点是 API 服务器,因为它允许我们查看是否存在客户端应用程序或控制平面的问题。]
