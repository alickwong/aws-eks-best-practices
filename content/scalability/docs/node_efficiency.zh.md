<translation>
!!! note
这个页面的内容是使用大型语言模型(Claude 3)创建的,基于英文版本。如有差异,以英文版本为准。

---
date: 2023-09-22
authors:
  - Shane Corbett
---
# 节点和工作负载效率
提高工作负载和节点的效率可以降低复杂性/成本,同时提高性能和扩展性。在规划这种效率时需要考虑许多因素,最简单的方法是权衡各种功能的利弊。让我们在以下部分深入探讨这些权衡。

## 节点选择
使用稍大一些的节点尺寸(4-12xlarge)可以增加我们运行 pod 的可用空间,因为它减少了节点用于"开销"的百分比,如 [DaemonSets](https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/) 和 [Reserves](https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/) 用于系统组件。在下图中,我们可以看到 2xlarge 和 8xlarge 系统之间的可用空间差异,只有适度数量的 DaemonSets。

!!! note
由于 k8s 通常是水平扩展的,对于大多数应用程序来说,采用 NUMA 大小的节点并不合理,因此建议使用以下范围内的节点大小。

![节点大小](../images/node-size.png)

大节点尺寸允许我们每个节点有更高的可用空间百分比。但是,这种模型可能会被过度利用,导致错误或饱和节点。监控节点饱和度是成功使用较大节点尺寸的关键。

节点选择很少是一刀切的。通常最好将具有不同变化率的工作负载分成不同的节点组。具有高变化率的小批量工作负载最适合 4xlarge 系列实例,而像 Kafka 这样的大规模应用程序(需要 8 个 vCPU,变化率低)更适合 12xlarge 系列。

![变化率](../images/churn-rate.png)

!!! tip
考虑使用非常大的节点尺寸的另一个因素是,由于 CGROUPS 不会隐藏容器化应用程序的总 vCPU 数,动态运行时可能会意外生成大量 OS 线程,从而导致难以排查的延迟。对于这些应用程序,[CPU 绑定](https://kubernetes.io/docs/tasks/administer-cluster/cpu-management-policies/#static-policy)是推荐的。有关此主题的更深入探讨,请参见以下视频 https://www.youtube.com/watch?v=NqtfDy_KAqg

## 节点装箱
### Kubernetes 与 Linux 规则
在 Kubernetes 上处理工作负载时,我们需要注意两组规则。Kubernetes 调度程序使用请求值来调度 pod 到节点,然后 pod 被调度后,就进入了 Linux 的领域,而不再是 Kubernetes 的。

Kubernetes 调度程序完成后,一组新的规则就开始生效,即 Linux 的完全公平调度器(CFS)。关键的启示是,Linux CFS 没有核心的概念。我们将讨论为什么以核心为中心的思维方式会导致优化工作负载扩展性的重大问题。

### 以核心为中心的思维
困惑开始于 Kubernetes 调度程序确实有核心的概念。从 Kubernetes 调度程序的角度来看,如果我们看到一个节点有 4 个 NGINX pod,每个 pod 都设置了一个核心的请求,那么节点看起来会像这样。

![](../images/cores-1.png)

但是,让我们通过一个思维实验来看看这在 Linux CFS 的角度上是多么不同。使用 Linux CFS 系统时最重要的事情是:繁忙的容器(CGROUPS)是唯一计入共享系统的容器。在这种情况下,只有第一个容器是繁忙的,所以它被允许使用节点上的所有 4 个核心。

![](../images/cores-2.png)

为什么这很重要?假设我们在开发集群中运行性能测试,NGINX 应用程序是该节点上唯一繁忙的容器。当我们将应用程序移到生产环境时,会发生以下情况:NGINX 应用程序需要 4 个 vCPU 的资源,但由于节点上的其他 pod 都很繁忙,我们的应用程序性能受到限制。

![](../images/cores-3.png)

这种情况会导致我们不必要地添加更多容器,因为我们没有让我们的应用程序扩展到它们的"最佳点"。让我们更详细地探讨一下这个"最佳点"的重要概念。

### 应用程序合理调整
每个应用程序都有一个特定的点,它无法再接受更多流量。超过这个点会增加处理时间,甚至在大幅超过这个点时会丢弃流量。这被称为应用程序的饱和点。为了避免扩展问题,我们应该在应用程序达到饱和点之前尝试扩展它。让我们称这个点为最佳点。

![最佳点](../images/sweet-spot.png)

我们需要测试每个应用程序,以了解其最佳点。这里不会有普遍的指导,因为每个应用程序都不同。在这个测试过程中,我们试图了解显示应用程序饱和点的最佳指标。通常使用利用率指标来表示应用程序已饱和,但这可能会快速导致扩展问题(我们将在后续部分详细探讨这个主题)。一旦我们有了这个"最佳点",我们就可以用它来有效地扩展我们的工作负载。

相反,如果我们在最佳点之前大幅扩展并创建不必要的 pod 会发生什么?让我们在下一节探讨一下。

### Pod 泛滥
为了看看创建不必要的 pod 会如何迅速失控,让我们看看左边的第一个例子。这个容器的正确垂直扩展占用了大约两个 vCPU 的利用率,可以处理每秒 100 个请求。但是,如果我们通过将请求值设置为半个核心来低估了请求,我们现在需要 4 个 pod 来替代我们实际需要的 1 个 pod。进一步加剧这个问题的是,如果我们的 [HPA](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/) 设置为默认的 50% CPU,这些 pod 会以一半的空间进行扩展,创造一个 8:1 的比率。

![](../images/scaling-ratio.png)

放大这个问题,我们可以很快看到它会失控。一个部署有十个 pod,其最佳点设置不正确,可能会迅速膨胀到 80 个 pod 和运行它们所需的额外基础设施。

![](../images/bad-sweetspot.png)

现在我们了解了不让应用程序在其最佳点运行的影响,让我们回到节点层面,问一下 Kubernetes 调度程序和 Linux CFS 之间的差异为什么如此重要?

当使用 HPA 进行扩展和缩减时,我们可能会有大量空间来分配更多 pod。这将是一个错误的决定,因为左边描述的节点已经达到 100% CPU 利用率。在一个不太现实但理论上可能的场景中,我们可能会有完全相反的情况,即我们的节点完全满了,但 CPU 利用率为零。

![](../images/hpa-utilization.png)

### 设置请求
将请求设置为该应用程序的"最佳点"值很诱人,但这会导致如下图所示的低效率。这里我们将请求值设置为 2 个 vCPU,但这些 pod 的平均利用率大部分时间只有 1 个 CPU。这种设置会导致我们浪费 50% 的 CPU 周期,这是不可接受的。

![](../images/requests-1.png)

这给了我们一个复杂的答案。容器利用率不能独立考虑;必须考虑节点上运行的其他应用程序。在下面的示例中,突发性质的容器与两个 CPU 利用率较低但可能受内存限制的容器混合在一起。这样,我们允许容器达到最佳点,而不会过度利用节点。

![](../images/requests-2.png)

从所有这些中吸取的重要概念是,使用 Kubernetes 调度程序的核心概念来理解 Linux 容器性能可能会导致决策不当,因为它们并不相关。

!!! tip
Linux CFS 有其优点。这对于基于 I/O 的工作负载特别有用。但是,如果您的应用程序使用完整的核心而没有边车,并且没有 I/O 要求,CPU 绑定可以大大减少这个过程的复杂性,并且在这些前提下是鼓励的。

## 利用率与饱和度
应用程序扩展中的一个常见错误是仅使用 CPU 利用率作为扩展指标。在复杂的应用程序中,这几乎总是一个很差的指标,表明应用程序实际上已经饱和请求。在左边的示例中,我们看到所有请求实际上都命中了 web 服务器,因此 CPU 利用率与饱和度很好地跟踪。

在真实世界的应用程序中,很可能有一些请求将由数据库层或身份验证层提供服务。在这种更常见的情况下,请注意 CPU 并没有与饱和度保持一致,因为请求正在由其他实体提供服务。在这种情况下,CPU 是一个非常糟糕的饱和指标。

![](../images/util-vs-saturation-1.png)

在应用程序性能中使用错误的指标是 Kubernetes 中不必要和不可预测扩展的首要原因。在为您使用的应用程序类型选择正确的饱和指标时,必须格外小心。值得注意的是,没有一种一刀切的建议可以给出。根据所使用的语言和应用程序类型的不同,饱和度有一组不同的指标。

我们可能会认为这个问题只存在于 CPU 利用率,但其他常见指标,如每秒请求数,也会陷入与上述相同的问题。请注意,请求也可能转到 DB 层、身份验证层,而不是直接由我们的 web 服务器提供服务,因此它不是 web 服务器本身饱和度的良好指标。

![](../images/util-vs-saturation-2.png)

不幸的是,在选择正确的饱和指标方面没有简单的答案。以下是一些需要考虑的指导方针:

* 了解您的语言运行时 - 具有多个 OS 线程的语言将与单线程应用程序产生不同的反应,从而影响节点的不同。
* 了解正确的垂直扩展 - 在扩展新 pod 之前,您希望应用程序的垂直扩展有多大的缓冲区?
* 什么指标真正反映了应用程序的饱和度 - Kafka Producer 的饱和指标将与复杂 web 应用程序大不相同。
* 节点上的其他应用程序如何相互影响 - 应用程序性能不是在真空中完成的,节点上的其他工作负载有很大影响。

总结这一部分,将 CPU 利用率视为我们节点和应用程序性能的唯一维度是很容易的。将 CPU 利用率作为我们节点和应用程序健康状况的唯一指标会在扩展、性能和成本方面创造问题,这些都是紧密相关的概念。应用程序和节点越高效,需要扩展的就越少,从而降低成本。

找到并使用适合您特定应用程序的正确饱和指标,也可以让您监控和报警该应用程序的真正瓶颈。如果跳过这个关键步骤,性能问题的报告将很难,如果不是不可能理解的话。

## 设置 CPU 限制
为了总结这一节对误解主题的探讨,我们将讨论 CPU 限制。简而言之,限制是与容器相关的元数据,它有一个每 100 毫秒重置一次的计数器。这有助于 Linux 跟踪特定容器在 100 毫秒内使用的总 CPU 资源。

![CPU 限制](../images/cpu-limits.png)

设置限制的一个常见错误是假设应用程序是单线程的,只在其"分配"的 vCPU 上运行。在上一节中,我们了解到 CFS 不分配核心,实际上运行大型线程池的容器将在节点上的所有可用 vCPU 上进行调度。

如果 64 个 OS 线程跨 64 个可用核心(从 Linux 节点的角度来看)运行,我们在 100 毫秒内累积的总 CPU 时间使用量将相当大。由于这可能只发生在垃圾收集过程中,很容易错过这种情况。这就是为什么有必要使用指标来确保我们在尝试设置限制之前有正确的长期使用情况。

幸运的是,我们有一种方法可以准确地看到应用程序的所有线程使用了多少 vCPU。我们将使用指标 `container_cpu_usage_seconds_total` 来实现这一目的。

由于节流逻辑每 100 毫秒发生一次,而这个指标是每秒指标,我们将使用 PromQL 来匹配这个 100 毫秒的周期。如果您想深入探讨这个 PromQL 语句的工作原理,请参见以下[博客](https://aws.amazon.com/blogs/containers/using-prometheus-to-avoid-disasters-with-kubernetes-cpu-limits/)。

PromQL 查询:

```
topk(3, max by (pod, container)(rate(container_cpu_usage_seconds_total{image!="", instance="$instance"}[$__rate_interval]))) / 10
```

![](../images/cpu-1.png)

一旦我们觉得有了正确的值,就可以将限制放到生产环境中。然后,我们需要查看应用程序是否由于意外原因而受到节流。我们可以通过查看 `container_cpu_throttled_seconds_total` 来实现这一点。

```
topk(3, max by (pod, container)(rate(container_cpu_cfs_throttled_seconds_total{image!=``""``, instance=``"$instance"``}[$__rate_interval]))) / 10
```

![](../images/cpu-2.png)

### 内存
内存分配是另一个很容易将 Kubernetes 调度行为与 Linux CGroup 行为混淆的例子。这是一个更微妙的话题,因为 Linux 在 CGroup v2 中处理内存的方式发生了重大变化,Kubernetes 也已经改变了它的语法来反映这一变化;请阅读这篇[博客](https://kubernetes.io/blog/2021/11/26/qos-memory-resources/)以获取更多细节。

与 CPU 请求不同,内存请求在调度过程完成后就不会被使用。这是因为我们无法像处理 CPU 那样压缩内存在 CGroup v1 中。这使我们只剩下内存限制,它们被设计为内存泄漏的安全网,通过完全终止 pod 来实现。这是一种非黑即白的方式,但现在我们有了新的方法来解决这个问题。

首先,重要的是要理解为容器设置正确的内存量并不像看起来那么简单。Linux 的文件系统会使用内存作为缓存来提高性能。这个缓存会随时间增长,很难知道有多少内存只是对缓存有好处,但可以在不显著影响应用程序性能的情况下回收。这常常导致误解内存使用情况。

能够"压缩"内存是 CGroup v2 的主要驱动力之一。有关 CGroup V2 成为必要的历史原因,请参见 Chris Down 在 LISA21 上的[演讲](https://www.youtube.com/watch?v=kPMZYoRxtmg),他在其中解释了无法正确设置最小内存是他创建 CGroup v2 和压力衰竭指标的原因之一。

幸运的是,Kubernetes 现在有了 `memory.min` 和 `memory.high` 的概念,作为 `requests.memory` 的一部分。这使我们能够更积极地释放这些缓存内存供其他容器使用。一旦容器达到内存高限制,内核就可以积极回收该容器的内存,直到达到设置的 `memory.min` 值。这为我们在节点遇到内存压力时提供了更多灵活性。

关键问题是,应该将 `memory.min` 设置为什么值?这就是内存压力衰竭指标发挥作用的地方。我们可以使用这些指标来检测容器级别的内存"抖动"。然后我们可以使用控制器,如 [fbtax](https://facebookmicrosites.github.io/cgroup2/docs/fbtax-results.html),通过检测这种内存抖动来确定 `memory.min` 的正确值,并动态设置 `memory.min` 值。

### 总结
总结一下这一部分,很容易混淆以下概念:

* 利用率和饱和度
* Linux 性能规则与 Kubernetes 调度程序逻辑

必须格外小心,保持这些概念的分离。性能和扩展性在深层次上是相互关联的。不必要的扩展会导致性能问题,反过来又会导致扩展问题。
</translation>