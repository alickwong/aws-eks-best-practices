
# Kubernetes 扩展理论

## 节点与变化率
在讨论 Kubernetes 的可扩展性时,我们通常会从集群中节点的数量着手。有趣的是,这通常并不是理解可扩展性最有用的指标。例如,一个有 5,000 个节点但 pod 数量固定的集群,在初始设置后不会给控制平面带来太大压力。但是,如果我们有一个 1,000 个节点的集群,在不到一分钟内尝试创建 10,000 个短暂的作业,这将给控制平面带来持续的压力。

仅使用节点数量来理解扩展性可能会产生误导。更好的方法是从变化率的角度来思考,即在特定时间段内(我们以 5 分钟为例,因为这是 Prometheus 查询的默认设置)发生的变化率。让我们探讨一下为什么从变化率的角度来定义问题,可以让我们更好地了解需要调整哪些参数来实现所需的扩展。

## 从查询每秒的角度思考
Kubernetes 为每个组件(Kubelet、调度器、Kube 控制器管理器和 API 服务器)都设置了一些保护机制,以防止压垮 Kubernetes 链中的下一个环节。例如,Kubelet 有一个标志可以限制对 API 服务器的调用速率。这些保护机制通常(但并非总是)以每秒查询数(QPS)的形式表达。

在更改这些 QPS 设置时必须格外小心。消除一个瓶颈(如 Kubelet 的每秒查询数)将会影响其他下游组件。这可能会在某个速率下压垮整个系统,所以理解和监控服务链的每个部分对于成功扩展 Kubernetes 工作负载至关重要。

!!! note 
    API 服务器有一个更复杂的系统,引入了 API 优先级和公平性,我们将单独讨论。

!!! note 
    小心,有些指标看起来很合适,但实际上在衡量其他东西。例如,`kubelet_http_inflight_requests` 只与 Kubelet 中的指标服务器有关,而不是 Kubelet 到 apiserver 的请求数。这可能会导致我们错误地配置 Kubelet 上的 QPS 标志。查看特定 Kubelet 的审核日志会是一种更可靠的方式来检查指标。

## 扩展分布式组件
由于 EKS 是一种托管服务,让我们将 Kubernetes 组件分为两类:AWS 托管组件(包括 etcd、Kube 控制器管理器和调度器,位于图左)和客户可配置组件(如 Kubelet、容器运行时以及调用 AWS API 的各种操作员,如网络和存储驱动程序,位于图右)。我们将 API 服务器放在中间,尽管它是 AWS 托管的,因为 API 优先级和公平性的设置可以由客户配置。

![Kubernetes 组件](../images/k8s-components.png)

## 上游和下游瓶颈
在监控每个服务时,我们需要从两个方向观察指标,以找出瓶颈。让我们以 Kubelet 为例来学习如何做到这一点。Kubelet 同时与 API 服务器和容器运行时进行通信;**我们需要监控什么,以及如何监控,来检测这两个组件是否存在问题?**

### 每个节点的 Pod 数量
当我们查看扩展数字(如每个节点可以运行多少个 pod)时,我们可能会直接接受上游支持的 110 个 pod 的数字。

!!! note
    https://kubernetes.io/docs/setup/best-practices/cluster-large/

但是,您的工作负载可能比在可扩展性测试中使用的更复杂。为了确保我们可以在生产环境中运行所需的 pod 数量,让我们确保 Kubelet 能够"跟上" Containerd 运行时。

![跟上](../images/keeping-up.png)

简单地说,Kubelet 从容器运行时(在我们的例子中是 Containerd)获取 pod 的状态。如果 pod 的状态变化太快,会发生什么?如果变化率太高,对容器运行时的请求可能会超时。

!!! note 
    Kubernetes 正在不断发展,这个子系统目前正在经历变化。https://github.com/kubernetes/enhancements/issues/3386

![流程](../images/flow.png)
![PLEG 持续时间](../images/PLEG-duration.png)

在上图中,我们看到一条平坦的线,表示 pod 生命周期事件生成持续时间指标已经达到超时值。如果您想在自己的集群中看到这一点,可以使用以下 PromQL 语法:

```
increase(kubelet_pleg_relist_duration_seconds_bucket{instance="$instance"}[$__rate_interval])
```

如果我们观察到这种超时行为,就知道我们已经将节点推到了它的能力极限。我们需要解决导致超时的原因,然后再继续。这可以通过减少每个节点的 pod 数量或查找可能导致大量重试(从而影响变化率)的错误来实现。重要的是要认识到,指标是了解节点是否能够处理分配的 pod 变化率的最佳方式,而不是使用固定的数字。

## 通过指标进行扩展
尽管使用指标来优化系统是一个老生常谈的概念,但在人们开始使用 Kubernetes 时,它常常被忽视。我们应该关注于找到能帮助我们发现系统瓶颈的指标,而不是关注特定的数字(例如 110 个 pod 每个节点)。了解这些指标的正确阈值可以让我们对系统的最佳配置充满信心。

### 变更的影响
一个可能让我们陷入麻烦的常见模式是,关注第一个看起来可疑的指标或日志错误。当我们发现 Kubelet 出现超时时,我们可能会尝试一些随机的事情,比如增加 Kubelet 每秒钟允许发送的速率等。然而,明智的做法是先看看错误下游的整体情况。*每次变更都要有目的,并有数据支持*。

Kubelet 的下游包括 Containerd 运行时(pod 错误)、DaemonSets 如存储驱动程序(CSI)和网络驱动程序(CNI)(它们与 EC2 API 通信)等。

![流程添加组件](../images/flow-addons.png)

让我们继续我们之前的例子,Kubelet 跟不上运行时。有许多地方我们可能会将节点打包得太满,从而触发错误。

![瓶颈](../images/bottlenecks.png)

在为工作负载设计合适的节点大小时,这些容易被忽视的信号可能会给系统施加不必要的压力,从而限制我们的扩展性和性能。

### 不必要错误的代价

Kubernetes 控制器擅长在出现错误条件时进行重试,但这也是有代价的。这些重试可能会增加 Kube 控制器管理器等组件的压力。在进行规模测试时,监控这些错误是一个重要的原则。

当出现较少错误时,我们更容易发现系统中的问题。通过定期确保我们的集群在重大操作(如升级)之前是无错误的,我们可以简化在发生意外事件时的故障排查日志。

#### 扩展我们的视野

在拥有数千个节点的大规模集群中,我们不想单独寻找瓶颈。在 PromQL 中,我们可以使用 topk 函数找到数据集中的最高值;K 是我们设置的想要获取的项目数量。这里我们使用三个节点,以了解集群中的所有 Kubelet 是否都饱和了。到目前为止,我们一直在关注延迟,现在让我们看看 Kubelet 是否丢弃了事件。

```
topk(3, increase(kubelet_pleg_discard_events{}[$__rate_interval]))
```

分解这个语句:

* 我们使用 Grafana 变量 `$__rate_interval` 来确保它获取四个样本。这避免了一个复杂的监控主题。
* `topk` 给我们只返回最高结果,数字 3 限制了结果数量。这是一个非常适用于集群范围指标的函数。
* `{}` 告诉我们没有过滤器,通常您会放置任何抓取规则的作业名称,但由于这些名称会有所不同,我们将其留空。

#### 将问题一分为二

要解决系统中的瓶颈,我们将采取找到一个指标来显示问题是在上游还是下游的方法,这样可以将问题一分为二。这也将成为我们展示指标数据的一个核心原则。

一个不错的起点是 API 服务器,因为它可以让我们看出是客户端应用程序还是控制平面存在问题。