
# Node 和工作负载效率
提高我们的工作负载和节点的效率可以降低复杂性/成本,同时提高性能和扩展性。在规划这种效率时需要考虑许多因素,最简单的方法是权衡各种功能的最佳实践设置。让我们在以下部分深入探讨这些权衡。

## 节点选择
使用稍大一些的节点尺寸(4-12xlarge)可以增加我们运行 pod 的可用空间,因为它减少了用于"开销"的节点百分比,例如 [DaemonSets](https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/) 和为系统组件保留的 [Reserves](https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/)。在下图中,我们可以看到 2xlarge 和 8xlarge 系统在仅有适度数量的 DaemonSets 的情况下可用空间的差异。

!!! note
    由于 k8s 通常是横向扩展的,对于大多数应用程序来说,采用 NUMA 大小的节点并不合理,因此建议使用下面列出的节点尺寸范围。

![节点大小](../images/node-size.png)

较大的节点尺寸可以让我们每个节点拥有更高比例的可用空间。但是,这种模式可能会被极端化,即在节点上打包太多 pod,从而导致错误或饱和节点。监控节点饱和度是成功使用较大节点尺寸的关键。

节点选择很少是一刀切的。通常最好将变化率差异很大的工作负载分成不同的节点组。具有高变化率的小批量工作负载最适合 4xlarge 系列实例,而像 Kafka 这样需要 8 个 vCPU 且变化率低的大规模应用程序则更适合 12xlarge 系列。

![变化率](../images/churn-rate.png)

!!! tip
    考虑使用非常大的节点尺寸的另一个因素是,由于 CGROUPS 不会隐藏容器化应用程序看到的总 vCPU 数,动态运行时可能会意外生成大量 OS 线程,从而导致难以排查的延迟。对于这些应用程序,建议使用 [CPU 绑定](https://kubernetes.io/docs/tasks/administer-cluster/cpu-management-policies/#static-policy)。如需深入探讨此主题,请参见以下视频 https://www.youtube.com/watch?v=NqtfDy_KAqg

## 节点装箱
### Kubernetes 与 Linux 规则
在 Kubernetes 上处理工作负载时,我们需要注意两组规则。一是 Kubernetes Scheduler 使用请求值来调度 pod 到节点上的规则,二是 pod 被调度后在 Linux 层面发生的事情,这属于 Kubernetes 之外的范畴。
[在Kubernetes调度程序完成后,一组新的规则接管,即Linux完全公平调度程序(CFS)。关键的启示是,Linux CFS没有核心的概念。我们将讨论为什么从核心的角度思考可能会导致优化工作负载扩展时出现重大问题。

### 从核心的角度思考
困惑开始于Kubernetes调度程序确实有核心的概念。从Kubernetes调度程序的角度来看,如果我们观察一个有4个NGINX pod的节点,每个pod都设置了一个核心的请求,那么该节点看起来就像这样。

![](../images/cores-1.png)

但是,让我们进行一个思维实验,看看从Linux CFS的角度来看这种情况有何不同。使用Linux CFS系统时需要记住的最重要的事情是:繁忙的容器(CGROUPS)是唯一计入共享系统的容器。在这种情况下,只有第一个容器是繁忙的,所以它被允许使用节点上的所有4个核心。

![](../images/cores-2.png)

为什么这很重要?假设我们在一个开发集群中进行性能测试,其中NGINX应用程序是该节点上唯一繁忙的容器。当我们将应用程序移到生产环境时,会发生以下情况:NGINX应用程序需要4个虚拟CPU的资源,但是由于节点上的其他pod都很繁忙,我们的应用程序的性能受到限制。

![](../images/cores-3.png)

这种情况会导致我们不必要地添加更多容器,因为我们没有让我们的应用程序扩展到它们的"最佳点"。让我们更详细地探讨一下这个"最佳点"的重要概念。

### 应用程序的合适大小
每个应用程序都有一个特定的点,它无法再接受更多流量。超过这个点会增加处理时间,甚至在大大超过这个点时会丢弃流量。这被称为应用程序的饱和点。为了避免扩展问题,我们应该在应用程序达到饱和点之前尝试扩展它。让我们称这个点为最佳点。

![最佳点](../images/sweet-spot.png)

我们需要测试每个应用程序,以了解它的最佳点。这里不会有统一的指导,因为每个应用程序都不同。在这种测试中,我们试图了解最能显示应用程序饱和点的指标。通常使用利用率指标来表示应用程序已饱和,但这可能会快速导致扩展问题(我们将在后续部分详细探讨这个话题)。一旦我们有了这个"最佳点",我们就可以用它来有效地扩展我们的工作负载。

相反,如果我们在最佳点之前大幅扩展并创建不必要的pod会发生什么?让我们在下一节中探讨一下。]

为了看看创建不必要的 pod 是如何迅速失控的,让我们看看左边的第一个例子。当每秒处理 100 个请求时,这个容器的正确垂直刻度占用了大约 2 个 vCPU 的利用率。但是,如果我们将请求值设置为半个核心,我们现在需要 4 个 pod 来替代我们实际需要的 1 个 pod。进一步加剧这个问题的是,如果我们的 [HPA](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/) 设置为默认的 50% CPU,这些 pod 将以一半的空间进行扩展,创造一个 8:1 的比率。

![](../images/scaling-ratio.png)

放大这个问题,我们可以快速看到它是如何失控的。一个 10 个 pod 的部署,如果设置不正确,可能会迅速膨胀到 80 个 pod,以及运行它们所需的额外基础设施。

![](../images/bad-sweetspot.png)

现在我们了解了不让应用程序在其最佳点运行的影响,让我们回到节点级别,并问为什么 Kubernetes 调度程序和 Linux CFS 之间的这种差异如此重要?

当使用 HPA 进行扩展时,我们可能会有大量空间来分配更多的 pod。这将是一个错误的决定,因为左边描述的节点已经达到 100% 的 CPU 利用率。在一个不现实但理论上可能的极端情况下,我们的节点可能已经完全满了,但 CPU 利用率为零。

![](../images/hpa-utilization.png)

### 设置请求

将请求设置为该应用程序的"最佳点"值很诱人,但这将导致如下图所示的低效率。在这里,我们将请求值设置为 2 个 vCPU,但这些 pod 的平均利用率大部分时间只有 1 个 CPU。这种设置将导致我们浪费 50% 的 CPU 周期,这是不可接受的。

![](../images/requests-1.png)

这带我们进入了这个问题的复杂答案。容器利用率不能独立考虑;必须考虑节点上运行的其他应用程序。在下面的示例中,具有突发性质的容器与两个 CPU 利用率较低但可能受内存限制的容器混合在一起。这样,我们允许容器达到其最佳点,而不会给节点带来负担。

![](../images/requests-2.png)

从所有这些中得出的重要概念是,使用 Kubernetes 调度程序的核心概念来理解 Linux 容器性能可能会导致决策失误,因为它们并不相关。

!!! tip
    Linux CFS 有其优点。这对于基于 I/O 的工作负载特别适用。但是,如果您的应用程序使用完整的核心而没有 sidecar,并且没有 I/O 要求,CPU 固定可以大大减少这个过程的复杂性,并且在这些前提下是鼓励的。

## 利用率与饱和度

应用程序扩展中的一个常见错误是仅使用 CPU 利用率作为扩展指标。在复杂的应用程序中,这几乎总是一个很差的指标,表明应用程序实际上已饱和请求。在左侧的示例中,我们看到所有请求实际上都在命中 Web 服务器,因此 CPU 利用率与饱和度很好地跟踪。

在现实世界的应用程序中,这些请求中的一些可能会由数据库层或身份验证层等其他实体提供服务。在这种更常见的情况下,请注意 CPU 并没有与饱和度跟踪,因为请求正在由其他实体提供服务。在这种情况下,CPU 是一个非常糟糕的饱和度指标。

![](../images/util-vs-saturation-1.png)

在应用程序性能中使用错误的指标是 Kubernetes 中不必要和不可预测扩展的首要原因。在为您使用的应用程序类型选择正确的饱和度指标时,必须格外小心。值得注意的是,没有一种一刀切的建议可以给出。根据所使用的语言和应用程序类型的不同,饱和度有一组不同的指标。

我们可能会认为这个问题只存在于 CPU 利用率,但其他常见的指标,如每秒请求数,也会遇到与上述相同的问题。请注意,请求也可能转到 DB 层、身份验证层,而不是直接由我们的 Web 服务器提供服务,因此它是一个很差的指标,无法真正反映 Web 服务器本身的饱和度。

![](../images/util-vs-saturation-2.png)

不幸的是,在选择正确的饱和度指标方面没有简单的答案。以下是一些需要考虑的指导原则:

* 了解您的语言运行时 - 具有多个操作系统线程的语言将与单线程应用程序有不同的反应,从而影响节点。
* 了解正确的垂直扩展 - 在扩展新 pod 之前,您希望应用程序的垂直扩展有多大缓冲区?
* 什么指标真正反映了应用程序的饱和度 - Kafka Producer 的饱和度指标将与复杂的 Web 应用程序有很大不同。
* 节点上的其他应用程序如何相互影响 - 应用程序性能不是在真空中完成的,节点上的其他工作负载会产生重大影响。

总结这一部分,将上述内容视为过于复杂和不必要是很容易的。事实上,我们可能正在经历一个问题,但由于我们关注了错误的指标,我们并不知道问题的真正性质。在下一节中,我们将看看这种情况可能会发生。

### 节点饱和度

现在我们已经探讨了应用程序饱和度,让我们从节点的角度来看这个概念。让我们看两个 100% 利用的 CPU,看看利用率与饱和度之间的区别。
左侧的虚拟 CPU 利用率为 100%,但是没有其他任务等待在该虚拟 CPU 上运行,因此从理论上讲,这是相当高效的。与此同时,在第二个示例中,我们有 20 个单线程应用程序等待被虚拟 CPU 处理。所有 20 个应用程序现在都会在等待轮到它们被虚拟 CPU 处理时经历某种延迟。换句话说,右侧的虚拟 CPU 已饱和。

如果我们仅关注利用率,就无法发现这个问题,反而可能将这种延迟归咎于网络等无关因素,从而走上错误的道路。

![](../images/node-saturation.png)

在任何给定时间增加节点上运行的 pod 总数时,查看饱和度指标而不仅仅是利用率指标非常重要,因为我们很容易错过节点已经过度饱和的事实。对于这项任务,我们可以使用下图中显示的压力阻塞信息指标。

PromQL - 阻塞 I/O
```
topk(3, ((irate(node_pressure_io_stalled_seconds_total[1m])) * 100))
```


![](../images/stalled-io.png)

!!! note
    有关压力失速指标的更多信息,请参见 https://facebookmicrosites.github.io/psi/docs/overview

通过这些指标,我们可以判断线程是否在等待 CPU,甚至每个线程是否都在等待内存或 I/O 等资源而处于停滞状态。例如,我们可以看到实例上每个线程在 1 分钟内等待 I/O 的百分比。
```
topk(3, ((irate(node_pressure_io_stalled_seconds_total[1m])) * 100))
```

使用这个指标,我们可以在上面的图表中看到,每个线程在等待 I/O 时都被阻塞了 45% 的时间,这意味着我们在那一分钟内浪费了所有这些 CPU 周期。了解这种情况的发生可以帮助我们收回大量的 vCPU 时间,从而使扩展更加高效。

### HPA V2
建议使用 HPA API 的 autoscaling/v2 版本。HPA API 的旧版本在某些边缘情况下可能会陷入停滞。它还仅限于在每个扩展步骤中将 pod 数量翻倍,这对需要快速扩展的小型部署来说会产生问题。

Autoscaling/v2 使我们能够更灵活地包括多个标准来进行扩展,并在使用自定义和外部指标(非 K8s 指标)时提供了很大的灵活性。

例如,我们可以根据三个值中的最高值进行扩展(见下文)。如果所有 pod 的平均利用率超过 50%,如果入口的每秒数据包超过 1,000 个,或者入口对象每秒超过 10,000 个请求,我们就会进行扩展。

!!! 注意
    这只是为了展示自动扩展 API 的灵活性,我们建议不要使用过于复杂的规则,因为这可能会在生产环境中难以排查。
```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: php-apache
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: php-apache
  minReplicas: 1
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 50
  - type: Pods
    pods:
      metric:
        name: packets-per-second
      target:
        type: AverageValue
        averageValue: 1k
  - type: Object
    object:
      metric:
        name: requests-per-second
      describedObject:
        apiVersion: networking.k8s.io/v1
        kind: Ingress
        name: main-route
      target:
        type: Value
        value: 10k
```


然而,我们了解到使用此类指标来评估复杂的Web应用程序存在危险。在这种情况下,我们最好使用自定义或外部指标来准确反映应用程序的饱和度与利用率。HPAv2允许根据任何指标进行扩缩,但我们仍需要找到并将该指标导出到Kubernetes以供使用。

例如,我们可以查看Apache中的活动线程队列计数。这通常会创造一个"更平滑"的扩缩配置文件(稍后会详细介绍这个术语)。如果一个线程处于活动状态,不管该线程是在等待数据库层还是在本地处理请求,只要应用程序的所有线程都在使用,这就是应用程序饱和的很好指标。

我们可以使用这种线程耗尽作为信号,创建一个具有完全可用线程池的新Pod。这也让我们能够控制在高流量期间应用程序可以吸收的缓冲区大小。例如,如果我们有一个总共10个线程池,在使用4个线程与8个线程时的扩缩差异会很大。设置为4的情况下,对于需要在高负载下快速扩缩的应用程序来说是合理的;而设置为8则在请求缓慢增加而非急剧增加的情况下,会更有效地利用资源。

![](../images/thread-pool.png)

我们所说的"平滑"扩缩指的是什么?注意下面的图表,我们使用CPU作为指标。这个部署中的Pods在短时间内从50个激增到250个,然后又立即缩减。这种高效率的扩缩是集群流失的主要原因。

![](../images/spiky-scaling.png)

注意当我们改用反映应用程序正确"甜蜜点"的指标(图表中间部分)时,我们能够平稳地进行扩缩。我们的扩缩现在是高效的,Pods可以充分利用我们通过调整请求设置提供的头部空间。现在,一组较小的Pods在完成之前数百个Pods所做的工作。实际数据显示,这是Kubernetes集群可扩展性的首要因素。

![](../images/smooth-scaling.png)

关键的启示是,CPU利用率只是应用程序和节点性能的一个维度。将CPU利用率作为节点和应用程序健康状况的唯一指标会导致扩缩、性能和成本方面的问题,这些概念都紧密相关。应用程序和节点的性能越好,需要扩缩的就越少,从而降低成本。
[为您的特定应用程序选择正确的饱和度指标并使用它们,也可以让您监控和报警该应用程序的真正瓶颈。如果跳过这个关键步骤,性能问题的报告将很难,如果不是不可能理解的。

## 设置 CPU 限制
为了完成这个关于误解话题的部分,我们将介绍 CPU 限制。简而言之,限制是与容器相关联的元数据,它有一个每 100 毫秒重置一次的计数器。这有助于 Linux 跟踪特定容器在 100 毫秒内在整个节点上使用了多少 CPU 资源。

![CPU 限制](../images/cpu-limits.png)

设置限制的一个常见错误是假设应用程序是单线程的,只在其"分配"的虚拟 CPU 上运行。在上一节中,我们了解到 CFS 不会分配内核,实际上,运行大型线程池的容器将在该主机上的所有可用虚拟 CPU 上进行调度。

如果 64 个操作系统线程在 64 个可用内核上运行(从 Linux 节点的角度来看),在所有这些 64 个内核上运行的时间加起来后,我们将在 100 毫秒内产生相当大的 CPU 时间总量。由于这可能只发生在垃圾回收过程中,很容易错过这种情况。这就是为什么有必要使用指标来确保我们在设置限制之前有正确的长期使用情况。

幸运的是,我们有一种方法可以准确地看到应用程序的所有线程使用了多少虚拟 CPU。我们将使用指标 `container_cpu_usage_seconds_total` 来实现这一目的。

由于节流逻辑每 100 毫秒发生一次,而这个指标是每秒指标,我们将使用 PromQL 来匹配这个 100 毫秒的周期。如果您想深入研究这个 PromQL 语句,请参见以下[博客](https://aws.amazon.com/blogs/containers/using-prometheus-to-avoid-disasters-with-kubernetes-cpu-limits/)。

PromQL 查询:]
```
topk(3, max by (pod, container)(rate(container_cpu_usage_seconds_total{image!="", instance="$instance"}[$__rate_interval]))) / 10
```

![](../images/cpu-1.png)

一旦我们觉得有了正确的值,我们就可以将限制投入生产。然后有必要查看我们的应用程序是否由于某些意外因素而被节流。我们可以通过查看 `container_cpu_throttled_seconds_total` 来做到这一点。
```
topk(3, max by (pod, container)(rate(container_cpu_cfs_throttled_seconds_total{image!=``""``, instance=``"$instance"``}[$__rate_interval]))) / 10
```


![](../images/cpu-2.png)
### 内存
内存分配是另一个例子,在这里很容易将 Kubernetes 调度行为与 Linux CGroup 行为混淆。这是一个更微妙的话题,因为 CGroup v2 在处理 Linux 内存方面发生了重大变化,Kubernetes 也已经改变了它的语法来反映这一变化;请阅读这篇[博客](https://kubernetes.io/blog/2021/11/26/qos-memory-resources/)以了解更多详情。

与 CPU 请求不同,内存请求在调度过程完成后就不再使用。这是因为我们无法像处理 CPU 那样压缩 CGroup v1 中的内存。这使我们只剩下内存限制,这些限制旨在作为内存泄漏的安全措施,通过完全终止 pod 来实现。这是一种非黑即白的方式,但现在我们已经有了新的方法来解决这个问题。

首先,重要的是要了解为容器设置正确的内存量并不像看起来那么简单。Linux 文件系统将使用内存作为缓存来提高性能。这个缓存将随时间增长,很难知道有多少内存只是对缓存有好处,但可以在不对应用程序性能产生重大影响的情况下回收。这通常会导致对内存使用的误解。

能够"压缩"内存是 CGroup v2 的主要驱动力之一。关于为什么需要 CGroup V2 的更多历史背景,请参见 Chris Down 在 LISA21 上的[演讲](https://www.youtube.com/watch?v=kPMZYoRxtmg),他在其中解释了无法正确设置最小内存是推动他创建 CGroup v2 和压力衰减指标的原因之一。

幸运的是,Kubernetes 现在有了 `memory.min` 和 `memory.high` 的概念,作为 `requests.memory` 的一部分。这使我们能够积极地释放这些缓存内存供其他容器使用。一旦容器达到内存高限制,内核就可以积极回收该容器的内存,直到达到 `memory.min` 设置的值。这样就给我们在节点遇到内存压力时提供了更多灵活性。

关键问题是,应该将 `memory.min` 设置为什么值?这就是内存压力衰减指标发挥作用的地方。我们可以使用这些指标来检测容器级别的内存"抖动"。然后我们可以使用像 [fbtax](https://facebookmicrosites.github.io/cgroup2/docs/fbtax-results.html) 这样的控制器,通过检测这种内存抖动来确定 `memory.min` 的正确值,并动态地设置 `memory.min` 值。

### 总结
总之,很容易混淆以下概念:

* 利用率和饱和度
* Linux 性能规则与 Kubernetes 调度器逻辑

必须格外小心,保持这些概念的分离。性能和扩展性在深层次上是相互关联的。不必要的扩展会导致性能问题,而这反过来又会导致扩展问题。
