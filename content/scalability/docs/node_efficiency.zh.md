---
日期: 2023-09-22
作者:
  - Shane Corbett
---
# Node和工作负载效率
提高工作负载和节点的效率可以降低复杂性/成本,同时提高性能和扩展性。在规划这种效率时需要考虑许多因素,最简单的方法是权衡取舍,而不是为每个功能设置一个最佳实践。让我们在以下部分深入探讨这些权衡取舍。

## 节点选择
使用稍大一些的节点尺寸(4-12xlarge)可以增加我们运行 pod 的可用空间,因为它减少了节点用于"开销"的百分比,例如[DaemonSets](https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/)和[Reserves](https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/)用于系统组件。在下图中,我们可以看到2xlarge和8xlarge系统之间的可用空间差异,只有适度数量的DaemonSets。

!!! note
    由于 k8s 通常是横向扩展,对于大多数应用程序来说,采用 NUMA 大小的节点并不合理,因此建议使用下面提到的节点尺寸范围。

![节点大小](../images/node-size.png)

大节点尺寸允许我们每个节点有更高的可用空间百分比。但是,这种模型可能会被极端化,即将节点打包得太满,从而导致错误或饱和节点。监控节点饱和度是成功使用较大节点尺寸的关键。

节点选择很少是一刀切的。通常最好将具有不同变化率的工作负载分成不同的节点组。具有高变化率的小批量工作负载最适合4xlarge实例系列,而像Kafka这样需要8个vCPU且变化率低的大规模应用程序更适合12xlarge实例系列。

![变化率](../images/churn-rate.png)

!!! tip
    考虑使用非常大的节点尺寸的另一个因素是,CGROUPS不会隐藏容器化应用程序看到的总vCPU数。动态运行时可能会无意中生成大量OS线程,从而导致难以排查的延迟。对于这些应用程序,[CPU绑定](https://kubernetes.io/docs/tasks/administer-cluster/cpu-management-policies/#static-policy)是推荐的。如需深入探讨此主题,请参见以下视频 https://www.youtube.com/watch?v=NqtfDy_KAqg

## 节点装箱
### Kubernetes vs. Linux规则
在处理Kubernetes上的工作负载时,我们需要注意两组规则。Kubernetes调度程序使用请求值来调度pod到节点上的规则,以及pod被调度后发生的情况,即Linux的领域,而不是Kubernetes。

Kubernetes调度程序完成后,一组新的规则就开始生效,即Linux完全公平调度器(CFS)。关键的启示是,Linux CFS没有核心的概念。我们将讨论为什么以核心为中心的思维方式会导致优化工作负载以实现扩展性的重大问题。

### 以核心为中心的思维
困惑开始于Kubernetes调度程序确实有核心的概念。从Kubernetes调度程序的角度来看,如果我们看到一个节点有4个NGINX pod,每个pod的请求都设置为一个核心,那么节点看起来会是这样的。

![](../images/cores-1.png)

但是,让我们进行一个思维实验,看看从Linux CFS的角度来看这种情况有多不同。使用Linux CFS系统时最重要的事情是:繁忙的容器(CGROUPS)是唯一计入共享系统的容器。在这种情况下,只有第一个容器是繁忙的,所以它被允许使用节点上的所有4个核心。

![](../images/cores-2.png)

为什么这很重要?假设我们在开发集群中运行性能测试,其中NGINX应用程序是该节点上唯一繁忙的容器。当我们将应用程序移到生产环境时,会发生以下情况:NGINX应用程序需要4个vCPU的资源,但由于节点上的所有其他pod都很繁忙,我们的应用程序性能受到限制。

![](../images/cores-3.png)

这种情况会导致我们不必要地添加更多容器,因为我们没有让应用程序扩展到它们的"最佳点"。让我们更详细地探讨一下这个"最佳点"的重要概念。

### 应用程序合理调整
每个应用程序都有一个特定的点,它无法再接受更多流量。超过这个点会增加处理时间,甚至在大大超过这个点时丢弃流量。这被称为应用程序的饱和点。为了避免扩展问题,我们应该在应用程序达到饱和点之前尝试扩展它。让我们称这个点为最佳点。

![最佳点](../images/sweet-spot.png)

我们需要测试每个应用程序,以了解其最佳点。这里不会有普遍的指导,因为每个应用程序都不同。在这个测试过程中,我们试图了解显示应用程序饱和点的最佳指标。通常使用利用率指标来表示应用程序已饱和,但这可能很快导致扩展问题(我们将在后续部分详细探讨这个主题)。一旦我们有了这个"最佳点",我们就可以用它来有效地扩展我们的工作负载。

相反,如果我们在最佳点之前大幅扩展并创建不必要的pod会发生什么?让我们在下一节探讨一下。

### Pod泛滥
为了看看创建不必要的pod可能会失控,让我们看看左边的第一个例子。这个容器的正确垂直扩展占用了大约两个vCPU的利用率,可以处理每秒100个请求。但是,如果我们将请求值设置为半个核心,我们现在需要4个pod来处理我们实际需要的每个pod。进一步加剧这个问题的是,如果我们的[HPA](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/)设置为默认的50%CPU,这些pod会以一半的空间进行扩展,创造一个8:1的比率。

![](../images/scaling-ratio.png)

扩展这个问题,我们可以很快看到它会失控。一个十个pod的部署,其最佳点设置不正确,可能会迅速膨胀到80个pod,以及运行它们所需的额外基础设施。

![](../images/bad-sweetspot.png)

现在我们了解了不让应用程序在其最佳点运行的影响,让我们回到节点层面,问一下Kubernetes调度程序和Linux CFS之间的差异为什么如此重要?

当使用HPA进行扩展和缩减时,我们可能会有很多空间来分配更多的pod。这将是一个错误的决定,因为左边描述的节点已经达到100%的CPU利用率。在一个不太现实但理论上可能的场景中,我们可能会有完全相反的情况,即我们的节点完全满了,但CPU利用率为零。

![](../images/hpa-utilization.png)

### 设置请求
将请求设置为该应用程序的"最佳点"值很诱人,但这会造成下图所示的低效。这里我们将请求值设置为2个vCPU,但这些pod的平均利用率大部分时间只有1个CPU。这种设置会导致我们浪费50%的CPU周期,这是不可接受的。

![](../images/requests-1.png)

这给了我们一个复杂的解决方案。容器利用率不能独立考虑;必须考虑节点上运行的其他应用程序。在下面的示例中,性质突发的容器与两个CPU利用率较低且可能受内存限制的容器混合在一起。这样,我们允许容器达到最佳点,而不会过度利用节点。

![](../images/requests-2.png)

从所有这些中吸取的重要概念是,使用Kubernetes调度程序的核心概念来理解Linux容器性能可能会导致决策不当,因为它们并不相关。

!!! tip
    Linux CFS有其优点。这对I/O密集型工作负载尤其如此。但是,如果您的应用程序使用完整的核心而没有边车,并且没有I/O要求,CPU绑定可以大大减少这个过程的复杂性,并且在这些前提下是鼓励的。

## 利用率vs.饱和度
应用程序扩展中的一个常见错误是仅使用CPU利用率作为扩展指标。在复杂的应用程序中,这几乎总是一个很差的指标,表明应用程序实际上已经饱和请求。在左边的示例中,我们看到所有请求实际上都在命中Web服务器,因此CPU利用率与饱和度很好地跟踪。

在真实世界的应用程序中,很可能有一些请求将由数据库层或身份验证层等服务。在这种更常见的情况下,请注意CPU并没有与饱和度保持一致,因为请求是由其他实体提供服务的。在这种情况下,CPU是一个非常糟糕的饱和度指标。

![](../images/util-vs-saturation-1.png)

在应用程序性能中使用错误的指标是Kubernetes中不必要和不可预测扩展的首要原因。在为您使用的应用程序类型选择正确的饱和度指标时,必须格外小心。值得注意的是,没有一种一刀切的建议可以给出。根据使用的语言和应用程序的类型,有一组不同的饱和度指标。

我们可能会认为这个问题只存在于CPU利用率,但其他常见指标,如每秒请求数,也会陷入与上述相同的问题。请注意,请求也可能转到DB层、身份验证层,而不是直接由我们的Web服务器提供服务,因此它是一个很差的指标,无法真正反映Web服务器本身的饱和度。

![](../images/util-vs-saturation-2.png)

不幸的是,当涉及到选择正确的饱和度指标时,没有简单的答案。以下是一些需要考虑的指南:

* 了解您的语言运行时 - 具有多个OS线程的语言将与单线程应用程序产生不同的反应,从而影响节点。
* 了解正确的垂直扩展 - 在扩展新pod之前,您希望应用程序的垂直扩展有多大缓冲?
* 什么指标真正反映了应用程序的饱和度 - Kafka生产者的饱和度指标将与复杂的Web应用程序大不相同。
* 节点上的其他应用程序如何相互影响 - 应用程序性能不是在真空中完成的,节点上的其他工作负载有很大影响。

为了结束这一部分,很容易将上述内容视为过于复杂和不必要。事实上,我们可能正在经历一个问题,但由于我们正在查看错误的指标,我们无法意识到问题的真正性质。在下一节中,我们将看看这种情况可能会发生。

### 节点饱和
现在我们已经探讨了应用程序饱和,让我们从节点的角度来看这个概念。让我们看两个100%利用的CPU,看看利用率和饱和度之间的区别。

左边的vCPU是100%利用的,但没有其他任务等待运行在这个vCPU上,所以从纯理论的角度来看,这是相当高效的。与此同时,我们有20个单线程应用程序等待被第二个示例中的vCPU处理。所有20个应用程序现在都会在等待轮到它们被vCPU处理时经历某种延迟。换句话说,右边的vCPU是饱和的。

如果我们只看利用率,我们不会看到这个问题,但我们可能会将这种延迟归咎于网络等无关因素,这会导致我们走上错误的道路。

![](../images/node-saturation.png)

在增加节点上同时运行的pod总数时,查看饱和度指标而不仅仅是利用率指标很重要,因为我们很容易错过节点已经过度饱和的事实。为此,我们可以使用压力信息指标,如下图所示。

PromQL - 停滞的I/O

```
topk(3, ((irate(node_pressure_io_stalled_seconds_total[1m])) * 100))
```

![](../images/stalled-io.png)

!!! note
    有关压力信息指标的更多信息,请参见 https://facebookmicrosites.github.io/psi/docs/overview

通过这些指标,我们可以知道线程是否在等待CPU,甚至如果箱子上的每个线程都在等待资源,如内存或I/O。例如,我们可以看到在1分钟内,箱子上的每个线程有45%的时间都在等待I/O。

```
topk(3, ((irate(node_pressure_io_stalled_seconds_total[1m])) * 100))
```

使用这个指标,我们可以看到上图中每个线程在那一分钟内有45%的时间都在等待I/O,这意味着我们在那一分钟内浪费了所有这些CPU周期。了解正在发生的情况可以帮助我们收回大量的vCPU时间,从而提高扩展效率。

### HPA V2
建议使用HPA API的autoscaling/v2版本。HPA API的旧版本可能会陷入某些边缘情况而无法扩展。它还限制了每个扩展步骤只能翻倍,这对需要快速扩展的小型部署造成了问题。

Autoscaling/v2允许我们更灵活地包括多个标准来进行扩展,并允许我们在使用自定义和外部指标(非K8s指标)时有很大的灵活性。

例如,我们可以根据三个值中的最高值进行扩展(见下文)。如果所有pod的平均利用率超过50%,如果入口的自定义指标每秒数据包超过1,000,或者入口对象每秒超过10,000个请求,我们就会进行扩展。

!!! note
    这只是为了展示自动扩展API的灵活性,我们不建议使用过于复杂的规则,因为它们在生产环境中可能很难排查。

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: php-apache
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: php-apache
  minReplicas: 1
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 50
  - type: Pods
    pods:
      metric:
        name: packets-per-second
      target:
        type: AverageValue
        averageValue: 1k
  - type: Object
    object:
      metric:
        name: requests-per-second
      describedObject:
        apiVersion: networking.k8s.io/v1
        kind: Ingress
        name: main-route
      target:
        type: Value
        value: 10k
```

然而,我们了解到对于复杂的Web应用程序使用这种指标是危险的。在这种情况下,我们最好使用准确反映应用程序饱和度而不是利用率的自定义或外部指标。HPAv2允许我们根据任何指标进行扩展,但我们仍然需要找到并导出该指标以供Kubernetes使用。

例如,我们可以查看Apache中的活动线程队列计数。这通常会创造一个"更平滑"的扩展配置文件(稍后会详细介绍这个术语)。如果一个线程处于活动状态,无论该线程是在等待数据库层还是在本地提供服务,只要应用程序的所有线程都在使用,它就是一个很好的饱和度指标。

我们可以使用这个线程耗尽作为创建新pod的信号,并带有一个完全可用的线程池。这也让我们能够控制在高流量期间我们希望在应用程序中保留多大的缓冲区。例如,如果我们有一个总共10个线程池,在4个线程使用时扩展与8个线程使用时扩展会对我们在扩展应用程序时可用的缓冲区产生重大影响。设置为4对于需要在大流量下快速扩展的应用程序来说是合理的,而设置为8对于由于请求随时间缓慢增加而不是急剧增加的应用程序来说更有效地利用资源。

![](../images/thread-pool.png)

我们所说的"平滑"扩展是什么意思?注意下图,我们使用CPU作为指标。这个部署中的pod在短时间内从50个激增到250个,然后立即缩减。这种高度低效的扩展是集群上出现大量抖动的主要原因。

![](../images/spiky-scaling.png)

注意当我们改用反映应用程序正确最佳点的指标(图表中间部分)时,我们能够平稳地扩展。我们的扩展现在是高效的,我们的pod被允许在调整请求设置提供的缓冲区内完全扩展。现在,一组较小的pod正在完成之前数百个pod所做的工作。实际数据显示,这是Kubernetes集群可扩展性的首要因素。

![](../images/smooth-scaling.png)

关键的启示是,CPU利用率只是应用程序和节点性能的一个维度。将CPU利用率作为我们的节点和应用程序的唯一健康指标会在扩展、性能和成本方面造成问题,这些都是紧密相关的概念。应用程序和节点的性能越好,需要扩展的就越少,从而降低成本。

找到并使用适合您特定应用程序的正确饱和度指标,也可以让您监控和报警该应用程序的真正瓶颈。如果跳过这个关键步骤,性能问题的报告将很难,如果不是不可能理解的。

## 设置CPU限制
为了总结这一节对误解的主题,我们将讨论CPU限制。简而言之,限制是与容器相关的元数据,它有一个每100ms重置一次的计数器。这有助于Linux跟踪特定容器在100ms期间使用的总CPU资源。

![CPU限制](../images/cpu-limits.png)

设置限制的一个常见错误是假设应用程序是单线程的,只在其"分配"的vCPU上运行。在上一节中,我们了解到CFS不分配核心,实际上运行大型线程池的容器将在节点上所有可用的vCPU上进行调度。

如果64个OS线程跨64个可用核心(从Linux节点的角度来看)运行,我们在100ms期间内使用的总CPU时间将会很大,因为所有这64个核心上的运行时间都被累加起来了。由于这可能只发生在垃圾收集过程中,很容易错过这种情况。这就是为什么有必要使用指标来确保我们在尝试设置限制之前有正确的长期使用情况。

幸运的是,我们有一种方法可以准确地看到应用程序的所有线程使用了多少vCPU。我们将使用指标`container_cpu_usage_seconds_total`来实现这一点。

由于节流逻辑每100ms发生一次,而这个指标是每秒指标,我们将使用PromQL来匹配这个100ms周期。如果您想深入探讨这个PromQL语句的工作原理,请参见以下[博客](https://aws.amazon.com/blogs/containers/using-prometheus-to-avoid-disasters-with-kubernetes-cpu-limits/)。

PromQL查询:

```
topk(3, max by (pod, container)(rate(container_cpu_usage_seconds_total{image!="", instance="$instance"}[$__rate_interval]))) / 10
```

![](../images/cpu-1.png)

一旦我们觉得有了正确的值,我们就可以将限制放到生产环境中。然后,我们需要查看应用程序是否由于意外原因而受到节流。我们可以通过查看`container_cpu_throttled_seconds_total`来实现这一点

```
topk(3, max by (pod, container)(rate(container_cpu_cfs_throttled_seconds_total{image!=``""``, instance=``"$instance"``}[$__rate_interval]))) / 10
```

![](../images/cpu-2.png)

### 内存
内存分配是另一个很容易将Kubernetes调度行为与Linux CGroup行为混淆的例子。这是一个更微妙的话题,因为Linux CGroup v2在处理内存方面发生了重大变化,Kubernetes也已经改变了它的语法来反映这一变化;请阅读这篇[博客](https://kubernetes.io/blog/2021/11/26/qos-memory-resources/)以获取更多详细信息。

与CPU请求不同,内存请求在调度过程完成后就不再使用。这是因为我们无法像压缩CPU那样压缩内存在CGroup v1中。这只留下了内存限制,它被设计为内存泄漏的一种安全措施,通过完全终止pod来实现。这是一种非黑即白的方式,但现在我们有了新的方法来解决这个问题。

首先,重要的是要了解为容器设置正确数量的内存并不像看起来那么简单。Linux的文件系统将使用内存作为缓存来提高性能。这个缓存会随时间增长,很难知道有多少内存只是对缓存有好处,但可以在不对应用程序性能产生重大影响的情况下回收。这常常导致误解内存使用情况。

能够"压缩"内存是CGroup v2的主要驱动力之一。有关CGroup V2成为必要的历史原因,请参见Chris Down在LISA21上的[演讲](https://www.youtube.com/watch?v=kPMZYoRxtmg),他在其中解释了无法正确设置最小内存是他创建CGroup v2和压力信息指标的原因之一。

幸运的是,Kubernetes现在有了`memory.min`和`memory.high`的概念,作为`requests.memory`的一部分。这使我们有了更多灵活性来积极释放这些缓存内存供其他容器使用。一旦容器达到内存高限制,内核就可以积极回收该容器的内存,直到达到设置的`memory.min`值。从而在节点遇到内存压力时给我们更多的灵活性。

关键问题是,应该将`memory.min`设置为什么值?这就是内存压力信息指标发挥作用的地方。我们可以使用这些指标来检测容器级别的内存"抖动"。然后我们可以使用像[fbtax](https://facebookmicrosites.github.io/cgroup2/docs/fbtax-results.html)这样的控制器来检测`memory.min`的正确值,方法是查找这种内存抖动,并动态地将`memory.min`值设置为这个设置。

### 总结
总结一下这一部分,很容易混淆以下概念:

* 利用率和饱和度
* Linux性能规则与Kubernetes调度器逻辑

必须格外小心,保持这些概念的分离。性能和扩展性在深层次上是相互关联的。不必要的扩展会造成性能问题,反过来又会造成扩展问题。